{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bayesian Model Averaging Logistic Regression\n\nIn this notebook we will use Bayesian Model Averaging (BMA) to understand a logistic regression problem.  The data coronary heart disease (0 = does not have CHD, 1 = has CHD), depending on a number of medical predictor variables.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport statsmodels.api as sm\nfrom statsmodels.tools import add_constant\nfrom itertools import combinations\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-06T06:52:27.292416Z","iopub.execute_input":"2022-04-06T06:52:27.29273Z","iopub.status.idle":"2022-04-06T06:52:28.623713Z","shell.execute_reply.started":"2022-04-06T06:52:27.292696Z","shell.execute_reply":"2022-04-06T06:52:28.62285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load the data, and check the head.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/coronary-heart-disease/CHDdata.csv')\ndf[\"famhist\"] = (df[\"famhist\"] == \"Present\")*1 # converts the famhit to 0 (no hist) and 1 (has hist)\n#df = df.drop([\"famhist\"], axis=1)\ndf.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-04-06T06:52:28.625498Z","iopub.execute_input":"2022-04-06T06:52:28.625881Z","iopub.status.idle":"2022-04-06T06:52:28.682978Z","shell.execute_reply.started":"2022-04-06T06:52:28.625843Z","shell.execute_reply":"2022-04-06T06:52:28.682239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop([\"chd\"], axis=1)\ny = df[\"chd\"]\n# building the model and fitting the data \nlog_reg = sm.Logit(y, add_constant(X)).fit()\nlog_reg.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:52:28.684192Z","iopub.execute_input":"2022-04-06T06:52:28.684643Z","iopub.status.idle":"2022-04-06T06:52:28.7871Z","shell.execute_reply.started":"2022-04-06T06:52:28.684608Z","shell.execute_reply":"2022-04-06T06:52:28.78638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seaborn visualization library\nimport seaborn as sns\n# Create the default pairplot\ng = sns.pairplot(df, hue=\"chd\", palette=\"tab10\", markers=[\"o\", \"D\"])","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:52:28.788239Z","iopub.execute_input":"2022-04-06T06:52:28.788704Z","iopub.status.idle":"2022-04-06T06:52:58.787524Z","shell.execute_reply.started":"2022-04-06T06:52:28.788653Z","shell.execute_reply":"2022-04-06T06:52:58.786467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Bayesian Model Averaging\nHere we define the class that will perform our BMA analysis.\n\nFor any model $M_i$ (each model is defined by the set of predictor varialbes being used in the model), Bayes theorem tells us that the probability for $M_i$ is\n\\begin{equation}\np(M_i|X,y)=\\frac{p(X,y|M_i)p(M_i)}{p(X,y)}.\n\\end{equation}\n\nUsing our previous formulas, this becomes,\n\\begin{equation}\np(M_i|X,y)=\\frac{e^{−\\text{BIC}_i/2}p(M_i)}{\\sum_k e^{−\\text{BIC}_k/2}p(M_k)}.\n\\end{equation}\n\nSo far, we have just done Bayesian analysis to compute a posterior probability distribution on the parameters.  But now we can do more with the 'averaging' part of BMA.\n\nThe probability for any predictor variable is the sum of the probabilities for all models contiaining that predictor variable, and the expected value for the coefficient of the predictor variable is the average value of the coefficient over all models containing the variable, weighted by the probability of each model.  That is,\n\\begin{equation}\np(X_k) = \\sum_{M_i \\text{such that } X_k\\in M_i} p(M_i|X,y),\n\\end{equation}\nand\n\\begin{equation}\nE[\\beta_k] = \\sum_{M_i \\text{such that } X_k\\in M_i} p(M_i|X,y)\\times \\beta_k^{(i)},\n\\end{equation}\nwhere $\\beta_k^{(i)}$ is the coefficient of $X_k$ in model $M_i$.","metadata":{}},{"cell_type":"markdown","source":"Here is code for a BMA class that will do the Bayeisan Model Averaging.  This is the same as the code from the Bayesian Model Averaging notebook https://www.kaggle.com/billbasener/bayesian-model-averaging-regression-tutorial-pt-2, but with that added capability to do logistic regression via the keyword RegType to \"Logit\".","metadata":{}},{"cell_type":"code","source":"from mpmath import mp\nmp.dps = 50\nclass BMA:\n    \n    def __init__(self, y, X, **kwargs):\n        # Setup the basic variables.\n        self.y = y\n        self.X = X\n        self.names = list(X.columns)\n        self.nRows, self.nCols = np.shape(X)\n        self.likelihoods = mp.zeros(self.nCols,1)\n        self.likelihoods_all = {}\n        self.coefficients_mp = mp.zeros(self.nCols,1)\n        self.coefficients = np.zeros(self.nCols)\n        self.probabilities = np.zeros(self.nCols)\n        # Check the max model size. (Max number of predictor variables to use in a model.)\n        # This can be used to reduce the runtime but not doing an exhaustive sampling.\n        if 'MaxVars' in kwargs.keys():\n            self.MaxVars = kwargs['MaxVars']\n        else:\n            self.MaxVars = self.nCols  \n        # Prepare the priors if they are provided.\n        # The priors are provided for the individual regressor variables.\n        # The prior for a model is the product of the priors on the variables in the model.\n        if 'Priors' in kwargs.keys():\n            if np.size(kwargs['Priors']) == self.nCols:\n                self.Priors = kwargs['Priors']\n            else:\n                print(\"WARNING: Provided priors error.  Using equal priors instead.\")\n                print(\"The priors should be a numpy array of length equal tot he number of regressor variables.\")\n                self.Priors = np.ones(self.nCols)  \n        else:\n            self.Priors = np.ones(self.nCols)  \n        if 'Verbose' in kwargs.keys():\n            self.Verbose = kwargs['Verbose'] \n        else:\n            self.Verbose = False \n        if 'RegType' in kwargs.keys():\n            self.RegType = kwargs['RegType'] \n        else:\n            self.RegType = 'LS' \n        \n    def fit(self):\n        # Perform the Bayesian Model Averaging\n        \n        # Initialize the sum of the likelihoods for all the models to zero.  \n        # This will be the 'normalization' denominator in Bayes Theorem.\n        likelighood_sum = 0\n        \n        # To facilitate iterating through all possible models, we start by iterating thorugh\n        # the number of elements in the model.  \n        max_likelihood = 0\n        for num_elements in range(1,self.MaxVars+1): \n            \n            if self.Verbose == True:\n                print(\"Computing BMA for models of size: \", num_elements)\n            \n            # Make a list of all index sets of models of this size.\n            Models_next = list(combinations(list(range(self.nCols)), num_elements)) \n             \n            # Occam's window - compute the candidate models to use for the next iteration\n            # Models_previous: the set of models from the previous iteration that satisfy (likelihhod > max_likelihhod/20)\n            # Models_next:     the set of candidate models for the next iteration\n            # Models_current:  the set of models from Models_next that can be consturcted by adding one new variable\n            #                    to a model from Models_previous\n            if num_elements == 1:\n                Models_current = Models_next\n                Models_previous = []\n            else:\n                idx_keep = np.zeros(len(Models_next))\n                for M_new,idx in zip(Models_next,range(len(Models_next))):\n                    for M_good in Models_previous:\n                        if(all(x in M_new for x in M_good)):\n                            idx_keep[idx] = 1\n                            break\n                        else:\n                            pass\n                Models_current = np.asarray(Models_next)[np.where(idx_keep==1)].tolist()\n                Models_previous = []\n                        \n            \n            # Iterate through all possible models of the given size.\n            for model_index_set in Models_current:\n                \n                # Compute the linear regression for this given model. \n                model_X = self.X.iloc[:,list(model_index_set)]\n                if self.RegType == 'Logit':\n                    model_regr = sm.Logit(self.y, model_X).fit(disp=0)\n                else:\n                    model_regr = OLS(self.y, model_X).fit()\n                \n                # Compute the likelihood (times the prior) for the model. \n                model_likelihood = mp.exp(-model_regr.bic/2)*np.prod(self.Priors[list(model_index_set)])\n                    \n                if (model_likelihood > max_likelihood/20):\n                    if self.Verbose == True:\n                        print(\"Model Variables:\",model_index_set,\"likelihood=\",model_likelihood)\n                    self.likelihoods_all[str(model_index_set)] = model_likelihood\n                    \n                    # Add this likelihood to the running tally of likelihoods.\n                    likelighood_sum = mp.fadd(likelighood_sum, model_likelihood)\n\n                    # Add this likelihood (times the priors) to the running tally\n                    # of likelihoods for each variable in the model.\n                    for idx, i in zip(model_index_set, range(num_elements)):\n                        self.likelihoods[idx] = mp.fadd(self.likelihoods[idx], model_likelihood, prec=1000)\n                        self.coefficients_mp[idx] = mp.fadd(self.coefficients_mp[idx], model_regr.params[i]*model_likelihood, prec=1000)\n                    Models_previous.append(model_index_set) # add this model to the list of good models\n                    max_likelihood = np.max([max_likelihood,model_likelihood]) # get the new max likelihood if it is this model\n                else:\n                    if self.Verbose == True:\n                        print(\"Model Variables:\",model_index_set,\"rejected by Occam's window\")\n                    \n\n        # Divide by the denominator in Bayes theorem to normalize the probabilities \n        # sum to one.\n        self.likelighood_sum = likelighood_sum\n        for idx in range(self.nCols):\n            self.probabilities[idx] = mp.fdiv(self.likelihoods[idx],likelighood_sum, prec=1000)\n            self.coefficients[idx] = mp.fdiv(self.coefficients_mp[idx],likelighood_sum, prec=1000)\n        \n        # Return the new BMA object as an output.\n        return self\n    \n    def predict(self, data):\n        data = np.asarray(data)\n        if self.RegType == 'Logit':\n            try:\n                result = 1/(1+np.exp(-1*np.dot(self.coefficients,data)))\n            except:\n                result = 1/(1+np.exp(-1*np.dot(self.coefficients,data.T)))\n        else:\n            try:\n                result = np.dot(self.coefficients,data)\n            except:\n                result = np.dot(self.coefficients,data.T)\n        \n        return result  \n        \n    def summary(self):\n        # Return the BMA results as a data frame for easy viewing.\n        df = pd.DataFrame([self.names, list(self.probabilities), list(self.coefficients)], \n             [\"Variable Name\", \"Probability\", \"Avg. Coefficient\"]).T\n        return df  ","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:52:58.79023Z","iopub.execute_input":"2022-04-06T06:52:58.790504Z","iopub.status.idle":"2022-04-06T06:52:58.940817Z","shell.execute_reply.started":"2022-04-06T06:52:58.790477Z","shell.execute_reply":"2022-04-06T06:52:58.939918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we split our data into input X dataframe and an output y datafram, and run our BMA analysis.","metadata":{}},{"cell_type":"code","source":"result = BMA(y,add_constant(X), RegType = 'Logit', Verbose=True).fit()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:52:58.943678Z","iopub.execute_input":"2022-04-06T06:52:58.943947Z","iopub.status.idle":"2022-04-06T06:52:59.553954Z","shell.execute_reply.started":"2022-04-06T06:52:58.94392Z","shell.execute_reply":"2022-04-06T06:52:59.552995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:52:59.55507Z","iopub.execute_input":"2022-04-06T06:52:59.555372Z","iopub.status.idle":"2022-04-06T06:52:59.568815Z","shell.execute_reply.started":"2022-04-06T06:52:59.55534Z","shell.execute_reply":"2022-04-06T06:52:59.568014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For comparison\nlog_reg.params","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:52:59.569713Z","iopub.execute_input":"2022-04-06T06:52:59.569959Z","iopub.status.idle":"2022-04-06T06:52:59.576281Z","shell.execute_reply.started":"2022-04-06T06:52:59.569935Z","shell.execute_reply":"2022-04-06T06:52:59.575696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = result.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:52:59.577159Z","iopub.execute_input":"2022-04-06T06:52:59.57757Z","iopub.status.idle":"2022-04-06T06:52:59.5902Z","shell.execute_reply.started":"2022-04-06T06:52:59.577546Z","shell.execute_reply":"2022-04-06T06:52:59.58883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r['Variable Name']","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:52:59.591444Z","iopub.execute_input":"2022-04-06T06:52:59.59192Z","iopub.status.idle":"2022-04-06T06:52:59.601848Z","shell.execute_reply.started":"2022-04-06T06:52:59.591886Z","shell.execute_reply":"2022-04-06T06:52:59.601147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r['Probability']","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:52:59.602728Z","iopub.execute_input":"2022-04-06T06:52:59.603056Z","iopub.status.idle":"2022-04-06T06:52:59.614149Z","shell.execute_reply.started":"2022-04-06T06:52:59.603032Z","shell.execute_reply":"2022-04-06T06:52:59.613133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(r['Probability'])","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:52:59.615599Z","iopub.execute_input":"2022-04-06T06:52:59.615992Z","iopub.status.idle":"2022-04-06T06:52:59.759087Z","shell.execute_reply.started":"2022-04-06T06:52:59.615932Z","shell.execute_reply":"2022-04-06T06:52:59.758485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort r dataframe to provide sorted input into barplot\nsorted_prob = r.sort_values(by=['Probability'])\nsorted_prob","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:52:59.760013Z","iopub.execute_input":"2022-04-06T06:52:59.760351Z","iopub.status.idle":"2022-04-06T06:52:59.772201Z","shell.execute_reply.started":"2022-04-06T06:52:59.76032Z","shell.execute_reply":"2022-04-06T06:52:59.771068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Q2(a) - bar chart using matplotlib.pyplot.bar to show probs for each factor\n\nsorted_prob = r.sort_values(by=['Probability'])\n\nfig = plt.figure(figsize = (10, 5))\nplt.bar(sorted_prob['Variable Name'], sorted_prob['Probability'], color ='blue', width = 0.7)\n\nplt.xlabel(\"Heart Disease Variables\")\nplt.ylabel(\"Probaility\")\nplt.title(\"Probabilities for Heart Disease Variables\")\n\n# See: https://matplotlib.org/3.5.0/api/_as_gen/matplotlib.pyplot.bar.html\n# See: https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0\n# See: https://www.geeksforgeeks.org/bar-plot-in-matplotlib/","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:52:59.773366Z","iopub.execute_input":"2022-04-06T06:52:59.773658Z","iopub.status.idle":"2022-04-06T06:52:59.941142Z","shell.execute_reply.started":"2022-04-06T06:52:59.773627Z","shell.execute_reply":"2022-04-06T06:52:59.940449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Q2(b) Plot Avg. Coefficient\n## 'const' coeff is large relative to other variables - plot is perhaps distorted\n## Try to drop 'const' to see if plot is more informative\n\n#sorted_prob = r.sort_values(by=['Avg. Coefficient'])\n\nfig = plt.figure(figsize = (10, 5))\nplt.barh(sorted_prob['Variable Name'], sorted_prob['Avg. Coefficient'], color ='blue', align='center')\n\nplt.xlabel(\"Heart Disease Variables\")\nplt.ylabel(\"Probaility\")\nplt.title(\"Probabilities for Heart Disease Variables\")","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:52:59.942436Z","iopub.execute_input":"2022-04-06T06:52:59.943002Z","iopub.status.idle":"2022-04-06T06:53:00.113542Z","shell.execute_reply.started":"2022-04-06T06:52:59.942963Z","shell.execute_reply":"2022-04-06T06:53:00.11292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Q2(b) Plot Avg. Coefficient\n\n# Drop ''const' intercept as it distorts plot\nr_noconst = r.drop(0, 0, inplace=False)\nr_noconst\n\nsorted_coeff = r_noconst.sort_values(by=['Avg. Coefficient'])\n\nfig = plt.figure(figsize = (10, 5))\nplt.barh(sorted_coeff['Variable Name'], sorted_coeff['Avg. Coefficient'], color ='green', align='center')\n\nplt.xlabel(\"Heart Disease Coefficients\")\nplt.ylabel(\"Avg. Coefficient\")\nplt.title(\"Avg. Coefficients for Heart Disease Variables\")","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:53:00.115065Z","iopub.execute_input":"2022-04-06T06:53:00.115908Z","iopub.status.idle":"2022-04-06T06:53:00.278924Z","shell.execute_reply.started":"2022-04-06T06:53:00.115865Z","shell.execute_reply":"2022-04-06T06:53:00.278113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardize Probability & Avg. Coefficients\n\nprint(r.mean())\nprint(r.std())","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:53:00.280033Z","iopub.execute_input":"2022-04-06T06:53:00.280341Z","iopub.status.idle":"2022-04-06T06:53:00.291295Z","shell.execute_reply.started":"2022-04-06T06:53:00.280308Z","shell.execute_reply":"2022-04-06T06:53:00.290198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = r.copy(deep=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:53:00.292413Z","iopub.execute_input":"2022-04-06T06:53:00.292741Z","iopub.status.idle":"2022-04-06T06:53:00.302015Z","shell.execute_reply.started":"2022-04-06T06:53:00.292708Z","shell.execute_reply":"2022-04-06T06:53:00.300947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf1['prob_mean'] = df1['Probability'].mean()\ndf1['diff_probMean'] = df1['Probability'] - df1['prob_mean']\ndf1['prob_sd'] = df1['Probability'].std()\ndf1['Std_Prob'] = df1['diff_probMean'] / df1['prob_sd']\ndf1","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:53:00.303119Z","iopub.execute_input":"2022-04-06T06:53:00.303502Z","iopub.status.idle":"2022-04-06T06:53:00.32537Z","shell.execute_reply.started":"2022-04-06T06:53:00.303478Z","shell.execute_reply":"2022-04-06T06:53:00.324619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf1['coeff_mean'] = df1['Avg. Coefficient'].mean()\ndf1['diff_coeffMean'] = df1['Avg. Coefficient'] - df1['coeff_mean']\ndf1['coeff_sd'] = df1['Avg. Coefficient'].std()\ndf1['Std_Coeff'] = df1['diff_coeffMean'] / df1['coeff_sd']\ndf1","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:53:00.326567Z","iopub.execute_input":"2022-04-06T06:53:00.326955Z","iopub.status.idle":"2022-04-06T06:53:00.350452Z","shell.execute_reply.started":"2022-04-06T06:53:00.32693Z","shell.execute_reply":"2022-04-06T06:53:00.349553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Q2(c)(1) - bar chart using matplotlib.pyplot.bar to show standardized probs for each factor\n\nsorted_prob = df1.sort_values(by=['Std_Prob'])\n\nfig = plt.figure(figsize = (10, 5))\nplt.bar(sorted_prob['Variable Name'], sorted_prob['Std_Prob'], color ='blue', width = 0.7)\n\nplt.xlabel(\"Standardized Value\")\nplt.ylabel(\"Standardized Probability\")\nplt.title(\"Standardized Probabilities for Heart Disease Variables\")\n\n# See: https://matplotlib.org/3.5.0/api/_as_gen/matplotlib.pyplot.bar.html\n# See: https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0\n# See: https://www.geeksforgeeks.org/bar-plot-in-matplotlib/","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:53:00.352402Z","iopub.execute_input":"2022-04-06T06:53:00.352707Z","iopub.status.idle":"2022-04-06T06:53:00.495124Z","shell.execute_reply.started":"2022-04-06T06:53:00.352657Z","shell.execute_reply":"2022-04-06T06:53:00.493955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Q2(c)(2) Plot Standardized Avg. Coefficient\n## 'const' coeff is large relative to other variables - plot is perhaps distorted\n## Try to drop 'const' to see if plot is more informative\n\n\nsorted_coeff = df1.sort_values(by=['Std_Coeff'])\n\nfig = plt.figure(figsize = (10, 5))\nplt.barh(sorted_coeff['Variable Name'], sorted_coeff['Std_Coeff'], color ='green', align='center')\n\nplt.xlabel(\"Standardized Value\")\nplt.ylabel(\"Standardized Avg. Coefficients\")\nplt.title(\"Standardized Avg. Coefficients for Heart Disease Variables\")","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:53:00.496155Z","iopub.execute_input":"2022-04-06T06:53:00.496358Z","iopub.status.idle":"2022-04-06T06:53:00.852401Z","shell.execute_reply.started":"2022-04-06T06:53:00.496337Z","shell.execute_reply":"2022-04-06T06:53:00.850926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Q2(c)(2): Do the plots change after standardizing data?\n\n## Yes, they do change. Standardizing variables converts the variables into standard deviations above or below the mean. The plots are more readable and understandable. Standardized variables are variables that have been transformed by subtracting the mean from every observed value and then dividing by the standard deviation. Because of the way they are constructed, standardized variables always have a mean of 0 and a standard deviation of 1. Standardized variables have no units. \n\n## Standardizing the variables means the plots more readable and understandable - especially with respect to the very large and very small variables.\n\n### See: https://en.wikibooks.org/wiki/Social_Statistics/Chapter_8","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.likelihoods","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:53:00.853815Z","iopub.execute_input":"2022-04-06T06:53:00.854117Z","iopub.status.idle":"2022-04-06T06:53:00.860066Z","shell.execute_reply.started":"2022-04-06T06:53:00.854086Z","shell.execute_reply":"2022-04-06T06:53:00.859106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Likelihoods for all of the models\nresult.likelihoods_all","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:53:00.861018Z","iopub.execute_input":"2022-04-06T06:53:00.861271Z","iopub.status.idle":"2022-04-06T06:53:00.870873Z","shell.execute_reply.started":"2022-04-06T06:53:00.861248Z","shell.execute_reply":"2022-04-06T06:53:00.870114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict the y-values from training input data\npred_BMA = result.predict(add_constant(X))\npred_Logit = log_reg.predict(add_constant(X))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:53:00.871832Z","iopub.execute_input":"2022-04-06T06:53:00.872112Z","iopub.status.idle":"2022-04-06T06:53:00.888963Z","shell.execute_reply.started":"2022-04-06T06:53:00.872081Z","shell.execute_reply":"2022-04-06T06:53:00.888152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the predictions with the actual values\n# subtract 0.05 so we can see the predictions more easily\nimport matplotlib.pyplot as plt\nplt.scatter(pred_BMA,y-0.05)\nplt.scatter(pred_Logit,y)\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"Coronary Heart Disease \\n(0=Not Present, 1=Present)\")\nplt.legend(['pred_BMA','pred_Logit'])","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:53:00.889984Z","iopub.execute_input":"2022-04-06T06:53:00.890313Z","iopub.status.idle":"2022-04-06T06:53:01.037154Z","shell.execute_reply.started":"2022-04-06T06:53:00.890286Z","shell.execute_reply":"2022-04-06T06:53:01.036654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" ","metadata":{}},{"cell_type":"code","source":"## Q2(d)(1) - BMA vs Logistic Regression Accuracy\n\n## The accuracy of both models is similr though BMA is slightly better.\n## This is as expected because we do not have a lot of collinearity as\n## observed from the scatter plots. If there was more collinearity, we\n## have seen more of a difference between because of the regularization\n## effects of BMA.\n\n# compute accuracy\nprint(\"BMA Accuracy: \", np.sum((pred_BMA > 0.5) == y)/len(y))\nprint(\"Logit Accuracy: \", np.sum((pred_Logit > 0.5) == y)/len(y))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:53:01.038105Z","iopub.execute_input":"2022-04-06T06:53:01.038444Z","iopub.status.idle":"2022-04-06T06:53:01.04493Z","shell.execute_reply.started":"2022-04-06T06:53:01.038409Z","shell.execute_reply":"2022-04-06T06:53:01.043988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Compare with test-train split","metadata":{}},{"cell_type":"code","source":"## Q2(d)(1) - BMA vs Logistic Regression Accuracy - modify test-train split\n## by testing different test/train splits and with/without random state.\n\n## The overall conclusion is that BMA becomes less accurate relative to\n## logistic regression (LR) as the test/train split changes: the lower the\n## training percentage (and the higher the test %), the less accurate BMA \n## is relative to LR. Eliminating the random state changes the difference\n## between the two accuracies, but the general conclusion still appears\n## to be true.\n\n## Running the test-train at 30%/70% (random state = 42) gives a similar \n## accuracy to above - a difference of 0.009. LR = .741 vs BMA = 0.742\n\n## Test/train 10%/90% - random state = 42 ==> LR = 0.723 vs BMA = 0.745\n## Test/train 10%/90% - no random state ==> LR = 0.638 vs BMA = 0.681\n\n## Test/train 50%/50% - random state = 42 ==> LR = 0.771 vs BMA = 0.729\n## Test/train 50%/50% - no random state ==> LR = 0.740 vs BMA = 0.714\n\n## Test/train 70%/30% - random state = 42 ==> LR = 0.719 vs BMA = 0.688\n## Test/train 70%/30% - no random state ==> LR = 0.725 vs BMA = 0.707 \n\n\n# build test-train split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.7, random_state=42)\n# trian on training data \nlog_reg = sm.Logit(y_train, add_constant(X_train)).fit()\nresult = BMA(y_train,add_constant(X_train), RegType = 'Logit', Verbose=False).fit()\n# predict the y-values from test data\npred_BMA = result.predict(add_constant(X_test))\npred_Logit = log_reg.predict(add_constant(X_test))\n# compute accuracy\nprint(\"BMA Accuracy: \", np.sum((pred_BMA > 0.5) == y_test)/len(y_test))\nprint(\"Logit Accuracy: \", np.sum((pred_Logit > 0.5) == y_test)/len(y_test))","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:53:01.046292Z","iopub.execute_input":"2022-04-06T06:53:01.046615Z","iopub.status.idle":"2022-04-06T06:53:01.501642Z","shell.execute_reply.started":"2022-04-06T06:53:01.046588Z","shell.execute_reply":"2022-04-06T06:53:01.500683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install lazypredict\nfrom lazypredict.Supervised import LazyClassifier, LazyRegressor","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:53:01.503416Z","iopub.execute_input":"2022-04-06T06:53:01.503734Z","iopub.status.idle":"2022-04-06T06:53:41.528255Z","shell.execute_reply.started":"2022-04-06T06:53:01.503708Z","shell.execute_reply":"2022-04-06T06:53:41.527164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit all models\nclf = LazyClassifier(predictions=True)\nmodels, predictions = clf.fit(X_train, X_test, y_train, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:53:41.529502Z","iopub.execute_input":"2022-04-06T06:53:41.529801Z","iopub.status.idle":"2022-04-06T06:53:42.557299Z","shell.execute_reply.started":"2022-04-06T06:53:41.529765Z","shell.execute_reply":"2022-04-06T06:53:42.556417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:53:42.558513Z","iopub.execute_input":"2022-04-06T06:53:42.559093Z","iopub.status.idle":"2022-04-06T06:53:42.579982Z","shell.execute_reply.started":"2022-04-06T06:53:42.559053Z","shell.execute_reply":"2022-04-06T06:53:42.579125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## from: https://towardsdatascience.com/exploratory-data-analysis-with-pandas-profiling-de3aae2ddff3 \nfrom IPython.display import IFrame\nimport IPython\nfrom pandas_profiling import ProfileReport\nprof = ProfileReport(df)\nprof.to_file(output_file='output_ProfileReport.html')","metadata":{"execution":{"iopub.status.busy":"2022-04-06T06:53:42.584425Z","iopub.execute_input":"2022-04-06T06:53:42.585073Z","iopub.status.idle":"2022-04-06T06:53:43.781265Z","shell.execute_reply.started":"2022-04-06T06:53:42.585032Z","shell.execute_reply":"2022-04-06T06:53:43.779952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}